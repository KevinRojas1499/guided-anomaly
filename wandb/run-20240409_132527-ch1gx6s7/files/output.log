Logging to /tmp/openai-2024-04-09-13-25-31-442104
Namespace(anneal_lr=True, batch_size=32, classifier_attention_resolutions='32,16,8', classifier_depth=2, classifier_pool='attention', classifier_resblock_updown=True, classifier_use_fp16=False, classifier_use_scale_shift_norm=True, classifier_width=128, data_dir='./Preprocessed/train', diffusion_steps=1000, eval_interval=5, image_size=128, iterations=100, learn_sigma=False, log_dir='./checkpoints', log_interval=10, lr=0.00022081382530106825, microbatch=-1, noise_schedule='linear', noised=True, predict_xstart=False, rescale_learned_sigmas=False, rescale_timesteps=False, resume_checkpoint='', save_interval=100, schedule_sampler='uniform', timestep_respacing='', use_kl=False, val_data_dir='', weight_decay=0.1)
./checkpoints
Logging to ./checkpoints
creating model and diffusion...
creating data loader...
creating optimizer...
training classifier model...
-----------------------------
| grad_norm      | 126      |
| param_norm     | 147      |
| samples        | 32       |
| step           | 0        |
| train_acc@1    | 0        |
| train_acc@1_q0 | 0        |
| train_acc@1_q1 | 0        |
| train_acc@1_q2 | 0        |
| train_acc@1_q3 | 0        |
| train_acc@5    | 0        |
| train_acc@5_q0 | 0        |
| train_acc@5_q1 | 0        |
| train_acc@5_q2 | 0        |
| train_acc@5_q3 | 0        |
| train_loss     | 7.09     |
| train_loss_q0  | 7        |
| train_loss_q1  | 7.13     |
| train_loss_q2  | 7.05     |
| train_loss_q3  | 7.16     |
-----------------------------
-----------------------------
| grad_norm      | 8.09     |
| param_norm     | 147      |
| samples        | 352      |
| step           | 10       |
| train_acc@1    | 0.234    |
| train_acc@1_q0 | 0.269    |
| train_acc@1_q1 | 0.198    |
| train_acc@1_q2 | 0.3      |
| train_acc@1_q3 | 0.187    |
| train_acc@5    | 1        |
| train_acc@5_q0 | 1        |
| train_acc@5_q1 | 1        |
| train_acc@5_q2 | 1        |
| train_acc@5_q3 | 1        |
| train_loss     | 3.77     |
| train_loss_q0  | 3.76     |
| train_loss_q1  | 3.55     |
| train_loss_q2  | 3.77     |
| train_loss_q3  | 3.98     |
-----------------------------
-----------------------------
| grad_norm      | 13.9     |
| param_norm     | 147      |
| samples        | 672      |
| step           | 20       |
| train_acc@1    | 0.259    |
| train_acc@1_q0 | 0.277    |
| train_acc@1_q1 | 0.244    |
| train_acc@1_q2 | 0.247    |
| train_acc@1_q3 | 0.267    |
| train_acc@5    | 1        |
| train_acc@5_q0 | 1        |
| train_acc@5_q1 | 1        |
| train_acc@5_q2 | 1        |
| train_acc@5_q3 | 1        |
| train_loss     | 1.47     |
| train_loss_q0  | 1.47     |
| train_loss_q1  | 1.47     |
| train_loss_q2  | 1.48     |
| train_loss_q3  | 1.47     |
-----------------------------
-----------------------------
| grad_norm      | 15.1     |
| param_norm     | 147      |
| samples        | 992      |
| step           | 30       |
| train_acc@1    | 0.294    |
| train_acc@1_q0 | 0.304    |
| train_acc@1_q1 | 0.289    |
| train_acc@1_q2 | 0.276    |
| train_acc@1_q3 | 0.31     |
| train_acc@5    | 1        |
| train_acc@5_q0 | 1        |
| train_acc@5_q1 | 1        |
| train_acc@5_q2 | 1        |
| train_acc@5_q3 | 1        |
| train_loss     | 1.44     |
| train_loss_q0  | 1.44     |
| train_loss_q1  | 1.48     |
| train_loss_q2  | 1.43     |
| train_loss_q3  | 1.41     |
-----------------------------
-----------------------------
| grad_norm      | 11.7     |
| param_norm     | 147      |
| samples        | 1.31e+03 |
| step           | 40       |
| train_acc@1    | 0.259    |
| train_acc@1_q0 | 0.26     |
| train_acc@1_q1 | 0.238    |
| train_acc@1_q2 | 0.291    |
| train_acc@1_q3 | 0.247    |
| train_acc@5    | 1        |
| train_acc@5_q0 | 1        |
| train_acc@5_q1 | 1        |
| train_acc@5_q2 | 1        |
| train_acc@5_q3 | 1        |
| train_loss     | 1.42     |
| train_loss_q0  | 1.45     |
| train_loss_q1  | 1.41     |
| train_loss_q2  | 1.4      |
| train_loss_q3  | 1.43     |
-----------------------------
-----------------------------
| grad_norm      | 14.4     |
| param_norm     | 147      |
| samples        | 1.63e+03 |
| step           | 50       |
| train_acc@1    | 0.291    |
| train_acc@1_q0 | 0.359    |
| train_acc@1_q1 | 0.2      |
| train_acc@1_q2 | 0.333    |
| train_acc@1_q3 | 0.292    |
| train_acc@5    | 1        |
| train_acc@5_q0 | 1        |
| train_acc@5_q1 | 1        |
| train_acc@5_q2 | 1        |
| train_acc@5_q3 | 1        |
| train_loss     | 1.44     |
| train_loss_q0  | 1.39     |
| train_loss_q1  | 1.44     |
| train_loss_q2  | 1.45     |
| train_loss_q3  | 1.46     |
-----------------------------
-----------------------------
| grad_norm      | 11.7     |
| param_norm     | 147      |
| samples        | 1.95e+03 |
| step           | 60       |
| train_acc@1    | 0.253    |
| train_acc@1_q0 | 0.162    |
| train_acc@1_q1 | 0.312    |
| train_acc@1_q2 | 0.305    |
| train_acc@1_q3 | 0.23     |
| train_acc@5    | 1        |
| train_acc@5_q0 | 1        |
| train_acc@5_q1 | 1        |
| train_acc@5_q2 | 1        |
| train_acc@5_q3 | 1        |
| train_loss     | 1.42     |
| train_loss_q0  | 1.45     |
| train_loss_q1  | 1.41     |
| train_loss_q2  | 1.38     |
| train_loss_q3  | 1.45     |
-----------------------------
-----------------------------
| grad_norm      | 6.15     |
| param_norm     | 147      |
| samples        | 2.27e+03 |
| step           | 70       |
| train_acc@1    | 0.284    |
| train_acc@1_q0 | 0.277    |
| train_acc@1_q1 | 0.286    |
| train_acc@1_q2 | 0.271    |
| train_acc@1_q3 | 0.307    |
| train_acc@5    | 1        |
| train_acc@5_q0 | 1        |
| train_acc@5_q1 | 1        |
| train_acc@5_q2 | 1        |
| train_acc@5_q3 | 1        |
| train_loss     | 1.38     |
| train_loss_q0  | 1.38     |
| train_loss_q1  | 1.36     |
| train_loss_q2  | 1.41     |
| train_loss_q3  | 1.38     |
-----------------------------
-----------------------------
| grad_norm      | 8.28     |
| param_norm     | 147      |
| samples        | 2.59e+03 |
| step           | 80       |
| train_acc@1    | 0.247    |
| train_acc@1_q0 | 0.205    |
| train_acc@1_q1 | 0.301    |
| train_acc@1_q2 | 0.272    |
| train_acc@1_q3 | 0.208    |
| train_acc@5    | 1        |
| train_acc@5_q0 | 1        |
| train_acc@5_q1 | 1        |
| train_acc@5_q2 | 1        |
| train_acc@5_q3 | 1        |
| train_loss     | 1.4      |
| train_loss_q0  | 1.4      |
| train_loss_q1  | 1.38     |
| train_loss_q2  | 1.38     |
| train_loss_q3  | 1.45     |
